
library(Rfast)
library(glmnet)
library(ranger)
library(datasets)
library(caret)
library(MASS)
# Helper packages
library(dplyr)         # for basic data wrangling
library(rootSolve)
library(gbm)
library(vtreat)
library(xgboost)

# Modeling packages
#library(keras)         # for fitting DNNs
#library(tfruns)        # for additional grid search & model training functions

# Modeling helper package - not necessary for reproducibility
#library(tfestimators)  # provides grid search & model training interface
#library(tensorflow)
# install_tensorflow()
#tf$constant("Hellow Tensorflow")


##first derivative of -log EL
R1der<-function(lambda,ZZ)
{
  apply(ZZ,2,function(xx)
  {as.matrix(xx,ncol=1)/as.vector((1+t(lambda)%*%as.matrix(xx,ncol=1)))})%*%rep(1,ncol(ZZ))
}

##second derivative of -log EL
R2der<-function(lambda,ZZ)
{
  r2der<-0
  for(i in 1:ncol(ZZ))
  {
    r2der_i<--as.matrix(ZZ[,i],ncol=1)%*%t(as.matrix(ZZ[,i],ncol=1))/as.vector(1+t(lambda)%*%as.matrix(ZZ[,i],ncol=1))^2
    r2der<-r2der+r2der_i
  }
  r2der
}

##-log EL
R0der<-function(lambda,ZZ)
{
  apply(ZZ,2, function (xx) {log(as.vector(1+t(lambda)%*%as.matrix(xx,ncol=1)))})%*%rep(1,ncol(ZZ))
}

#lambda for propensity scores
lambda_propensity<-function(ZZ)
{
  ZZ<-ZZ
  dim(ZZ)
  apply(ZZ,1,mean)
  
  gamma<-1
  c<-0
  lambda<-rep(0,nrow(ZZ))
  tol<-10e-8
  Delta_old<-0
  repeat{
    rl<-R1der(lambda,ZZ)
    rll<-R2der(lambda,ZZ) 
    Delta<--ginv(rll)%*%rl
    if(mean(abs(Delta))<tol | sum(Delta-Delta_old)==0)
    {break}else{
      repeat{
        mm<-0
        repeat{
          delta<-gamma*Delta
          index_1<-apply(ZZ,2,function (xx)
          {ifelse(1+t(lambda+delta)%*%as.matrix(xx,ncol=1)<=0,1,0)}
          )
          if (sum(index_1)>0)
          {gamma<-gamma/2
          mm<-mm+1}else{break}}
        index_2<-ifelse(R0der(lambda+delta,ZZ)-R0der(lambda,ZZ)<0,1,0)
        if (index_2==1)
        {gamma<-gamma/2}else{break}
      }
      Delta_old<-Delta
    }
    lambda<-lambda+delta
    c<-c+1
    gamma<-(c)^(-0.5)
  }
  lambda
}


#calculate multiple robust propensity score
multir.match<-function(weight.linear=weight.linear1,weight.lasso=weight.lasso1,weight.rf=weight.rf1,weight.gb=weight.gb1,
                       pred.y.linear=pred.y.linear.trt1,pred.y.lasso=pred.y.lasso.trt1,pred.y.rf=pred.y.rf.trt1,pred.y.gb=pred.y.gb.trt1,
                       trt=trt_est)
{
  ZZ_trt_propensity<-rbind(t(weight.linear[trt==1])-as.vector(mean(weight.linear)),
                           t(weight.lasso[trt==1])-as.vector(mean(weight.lasso)),
                           t(weight.rf[trt==1])-as.vector(mean(weight.rf)),
                           t(weight.gb[trt==1])-as.vector(mean(weight.gb)),
                           t(pred.y.linear[trt==1])-as.vector(mean(pred.y.linear)),
                           t(pred.y.lasso[trt==1])-as.vector(mean(pred.y.lasso)),
                           t(pred.y.rf[trt==1])-as.vector(mean(pred.y.rf)),
                           t(pred.y.gb[trt==1])-as.vector(mean(pred.y.gb))
                           )
  lambda_prop_trt<-lambda_propensity(ZZ_trt_propensity)
  num_trt<-sum(trt==1)
  pscore.multi<-apply(ZZ_trt_propensity,2,function(xx){1/(1+t(matrix(lambda_prop_trt,ncol=1))%*%xx)/num_trt})
  
  pscore.multi
}

#setups
seed = commandArgs(trailingOnly=TRUE)
seed = as.numeric(seed)
set.seed(seed)
N<-2000
n.image<-200
Sigma.image<-matrix(0.5,ncol=n.image,nrow=n.image)
diag(Sigma.image)<-1

#generate true mean values
num.true<-100000
y_control_all<-rep()
y_trt_all<-rep()
for(i in 1:num.true)
{
  x<-rmvnorm(N,rep(0,5),Sigma.image[1:5,1:5])
  #mu_control<-x[,1]+(x[,1]-0.5)^2+sin(x[,3])+ifelse(x[,4]>0.5,1,0)+x[,5]*abs(x[,2]-x[,3]) #change this a bit
  #mu_control<-0.5*(x[,1]+0.5*(x[,2])^2+x[,2]*x[,3]+x[,3]+ifelse(x[,4]>0,1,0)+x[,4]*sin(x[,5])) #changed
  mu_control<-0.5*(x[,1]+0.5*(x[,2])^2+x[,2]*x[,3]+x[,3]+ifelse(x[,4]>0.3,1,0)+x[,4]*ifelse(x[,5]>0,1,0)) #changed
  
  #mu_control<-x[,1]+x[,2]+x[,3]+x[,4]+x[,5]
  mu_trt<-0.5+mu_control+0.5*mu_control
  y_control<-rnorm(N,mean=mu_control,sd=1)
  y_trt<-rnorm(N,mean=mu_trt,sd=1)
  y_control_all<-c(y_control_all,mean(y_control))
  y_trt_all<-c(y_trt_all,mean(y_trt))
}
true_control<-mean(y_control_all)
true_trt<-mean(y_trt_all)

true<-c(true_control,true_trt)
true<-c(0.7906917,1.6861640) #0.8426789, 1.7640703 nonlinear
#true<-c(-0.0004085574, 0.4993815427) #-0.0004085574,  0.4993815427 linear

raw.all<-rep()
iptw.linear.all<-rep()
iptw.known.all<-rep()
aiptw.lasso.all<-rep()
aiptw.rf.all<-rep()
aiptw.linear.all<-rep()
aiptw.known.all<-rep()
aiptw.gb.all<-rep()
mr.all<-rep()
num.iter<-50
timestart<-Sys.time()
for(iter in 1:num.iter)
{
##generate confounding variables
x<-rmvnorm(N,rep(0,n.image),Sigma.image)
#mu_control<-x[,1]+(x[,1]-0.5)^2+sin(x[,3])+ifelse(x[,4]>0.5,1,0)+x[,5]*abs(x[,2]-x[,3]) #change this a bit
mu_control<-0.5*(x[,1]+0.5*(x[,2])^2+x[,2]*x[,3]+x[,3]+ifelse(x[,4]>0.3,1,0)+x[,4]*ifelse(x[,5]>0,1,0)) #changed
#mu_control<-x[,1]+x[,2]+x[,3]+x[,4]+x[,5] #simple linear

mu_trt<-0.5+mu_control+0.5*mu_control
y_control<-rnorm(N,mean=mu_control,sd=1)
y_trt<-rnorm(N,mean=mu_trt,sd=1)

##generate trt assignment
#mu_trt_assign<-(1+exp(1-0.5*x[,1]+0.5*x[,2]-0.2*(x[,2]-0.5)^2-0.5*sin(x[,3])-0.5*ifelse(x[,4]>0,1,0)-0.5*x[,5]*ifelse(x[,1]>0.5,1,0)))^(-1)
#mu_trt_assign<-(1+exp(1-0.5*x[,1]+0.5*x[,2]-0.1*(x[,2]-0.5)^2-0.5*sin(x[,3])-0.5*ifelse(x[,4]>0,1,0)))^(-1)
#mu_trt_assign<-(1+exp(-1*x[,1]-0.5*(x[,2]-0.5)^2+0.1*exp(abs(x[,3]))-1*ifelse(x[,4]>0,1,0)-1*x[,5]*ifelse(x[,1]>0.5,1,0)))^(-1)
#mu_trt_assign<-(1+exp(-0.5*x[,1]+0.2*(x[,2]-0.1)^2-0.5*abs(x[,3])+0.5*(x[,4])*x[,5]))^(-1)  #complicated
mu_trt_assign<-(1+exp(1-x[,1]-0.5*(x[,2])^2-abs(x[,3])+0.5*(x[,4])*x[,5]))^(-1)  #complicated
#mu_trt_assign<-(1+exp(0-0.5*x[,1]+0.5*x[,2]-0.5*(x[,3])+0.5*(x[,4])-0.5*x[,5]))^(-1) #simple linear

summary(mu_trt_assign)
trt<-rbinom(N,1,mu_trt_assign)

x_reg<-data.frame(cbind(x,trt))
x_reg_trt<-data.frame(cbind(x,trt=1))
x_reg_control<-data.frame(cbind(x,trt=0))

x_reg_linearreg<-data.frame(cbind(x,trt,x*trt))
x_reg_linearreg_trt<-data.frame(cbind(x,trt=1,x))
x_reg_linearreg_control<-data.frame(cbind(x,trt=0,0*x))




##generate observed outcome
y_obs<-y_trt
y_obs[trt==0]<-y_control[trt==0]

##no adjustment
fit<-glm(y_obs~trt)
coefficients(fit)

##mean estimation
raw.control<-mean(y_obs[trt==0]) #control
raw.trt<-mean(y_obs[trt==1]) #trt
raw.all<-cbind(raw.all,c(raw.control,raw.trt))

###################################
#######machine learning methods
###################################
###split data
sample.ind<-sample(1:N,N/2,replace = FALSE)
y_obs_train<-y_obs[sample.ind]
x_obs_train<-x[sample.ind,]
x_obs_reg_train<-x_reg[sample.ind,]
x_obs_reg_trt_train<-x_reg_trt[sample.ind,]
x_obs_reg_control_train<-x_reg_control[sample.ind,]
x_obs_linearreg_train<-x_reg_linearreg[sample.ind,]
x_obs_linearreg_trt_train<-x_reg_linearreg_trt[sample.ind,]
x_obs_linearreg_control_train<-x_reg_linearreg_control[sample.ind,]
trt_train<-trt[sample.ind]
#pred.y.linear.trt1<-pred.y.linear.trt[sample.ind]
#pred.y.linear.control1<-pred.y.linear.control[sample.ind]
#weight.linear1<-weight.linear[sample.ind]

y_obs_est<-y_obs[!(1:N %in% sample.ind)]
x_obs_est<-x[!(1:N %in% sample.ind),]
x_obs_reg_est<-x_reg[!(1:N %in% sample.ind),]
x_obs_reg_trt_est<-x_reg_trt[!(1:N %in% sample.ind),]
x_obs_reg_control_est<-x_reg_control[!(1:N %in% sample.ind),]
x_obs_linearreg_est<-x_reg_linearreg[!(1:N %in% sample.ind),]
x_obs_linearreg_trt_est<-x_reg_linearreg_trt[!(1:N %in% sample.ind),]
x_obs_linearreg_control_est<-x_reg_linearreg_control[!(1:N %in% sample.ind),]
trt_est<-trt[!(1:N %in% sample.ind)]
#pred.y.linear.trt2<-pred.y.linear.trt[!(1:N %in% sample.ind)]
#pred.y.linear.control2<-pred.y.linear.control[!(1:N %in% sample.ind)]
#weight.linear2<-weight.linear[!(1:N %in% sample.ind)]

# write.csv(y_obs_train,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/y_obs_train.csv")
# write.csv(x_obs_train,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/x_obs_train.csv")
# write.csv(trt_train,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/trt_train.csv")
# write.csv(y_obs_est,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/y_obs_est.csv")
# write.csv(x_obs_est,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/x_obs_est.csv")
# write.csv(trt_est,"OneDrive - University of Maryland School of Medicine/longterm_research/causal inference machine learning/multir_ml/trt_est.csv")

###AIPTW known
##first samples
aiptw.known.control1<-mean(y_obs_est*(1-trt_est)/(1-mu_trt_assign[!(1:N %in% sample.ind)])-(1-trt_est-(1-mu_trt_assign[!(1:N %in% sample.ind)]))/(1-mu_trt_assign[!(1:N %in% sample.ind)])*mu_control[!(1:N %in% sample.ind)]) #control
aiptw.known.trt1<-mean(y_obs_est*trt_est/mu_trt_assign[!(1:N %in% sample.ind)]-(trt_est-mu_trt_assign[!(1:N %in% sample.ind)])/mu_trt_assign[!(1:N %in% sample.ind)]*mu_trt[!(1:N %in% sample.ind)]) #trt

##second samples
aiptw.known.control2<-mean(y_obs_train*(1-trt_train)/(1-mu_trt_assign[sample.ind])-(1-trt_train-(1-mu_trt_assign[sample.ind]))/(1-mu_trt_assign[sample.ind])*mu_control[sample.ind]) #control
aiptw.known.trt2<-mean(y_obs_train*trt_train/mu_trt_assign[sample.ind]-(trt_train-mu_trt_assign[sample.ind])/mu_trt_assign[sample.ind]*mu_trt[sample.ind]) #trt

aiptw.known.all<-cbind(aiptw.known.all,c((aiptw.known.control1+aiptw.known.control2)/2,
                                           (aiptw.known.trt1+aiptw.known.trt2)/2))

###AIPTW linear
##first samples
fit<-glm(trt_train~x_obs_train,family = binomial(link="logit"))
weight.linear1<-(1+exp(-cbind(1,x_obs_est)%*%fit$coefficients))^(-1)

fit<-glm(y_obs_train~.,data=x_obs_linearreg_train)
pred.y.linear.trt1<-as.matrix(cbind(1,x_obs_linearreg_trt_est))%*%fit$coefficients
aiptw.linear.trt1=mean(y_obs_est*trt_est/weight.linear1-(trt_est-weight.linear1)/weight.linear1*pred.y.linear.trt1)
pred.y.linear.control1<-as.matrix(cbind(1,x_obs_linearreg_control_est))%*%fit$coefficients
aiptw.linear.control1=mean(y_obs_est*(1-trt_est)/(1-weight.linear1)-(1-trt_est-(1-weight.linear1))/(1-weight.linear1)*pred.y.linear.control1)

##second samples
fit<-glm(trt_est~x_obs_est,family = binomial(link="logit"))
weight.linear2<-(1+exp(-cbind(1,x_obs_train)%*%fit$coefficients))^(-1)

fit<-glm(y_obs_est~.,data=x_obs_linearreg_est)
pred.y.linear.trt2<-as.matrix(cbind(1,x_obs_linearreg_trt_train))%*%fit$coefficients
aiptw.linear.trt2=mean(y_obs_train*trt_train/weight.linear2-(trt_train-weight.linear2)/weight.linear2*pred.y.linear.trt2)
pred.y.linear.control2<-as.matrix(cbind(1,x_obs_linearreg_control_train))%*%fit$coefficients
aiptw.linear.control2=mean(y_obs_train*(1-trt_train)/(1-weight.linear2)-(1-trt_train-(1-weight.linear2))/(1-weight.linear2)*pred.y.linear.control2)

aiptw.linear.all<-cbind(aiptw.linear.all,c((aiptw.linear.control1+aiptw.linear.control1)/2,
                                           (aiptw.linear.trt1+aiptw.linear.trt2)/2))


###AIPTW Preg
######for the first set
##iptw
cvfit <-cv.glmnet(x=x_obs_train, y=trt_train,family="binomial",nfolds=5,alpha=0)
# w3 <- 1/abs(matrix(coef(cvfit, s=cvfit$lambda.min)
#                    [, 1][2:(ncol(x)+1)] ))^1
# w3[w3[,1] == Inf] <- 999999999 ## Replacing values estimated as Infinite for 999999999
# cv.lasso <- cv.glmnet(x=x_obs_train, y=trt_train, family='binomial', type.measure='auc', penalty.factor=w3)
est.parameter<-coef(cvfit, s = "lambda.min")

# group<-rep(1:n.image)
# fit<-grpreg(X=x_obs_train, y=trt_train, group=group, penalty="grLasso", alpha = 0.5,family="binomial")
# est.parameter<-select(fit,"AIC")$beta

weight<-(1+exp(-cbind(1,x_obs_est)%*%est.parameter))^(-1)
weight.lasso1<-weight

##regression
cvfit <-cv.glmnet(x=as.matrix(x_obs_linearreg_train), y=y_obs_train,nfolds=5,alpha=0)
est.parameter<-coef(cvfit, s = "lambda.min")
pred.y.lasso.control1<-as.matrix(cbind(1,x_obs_linearreg_control_est))%*%est.parameter
aiptw.lasso.control.1<-mean(y_obs_est*(1-trt_est)/(1-weight)-(1-trt_est-(1-weight))/(1-weight)*pred.y.lasso.control1) #control
pred.y.lasso.trt1<-as.matrix(cbind(1,x_obs_linearreg_trt_est))%*%est.parameter
aiptw.lasso.trt.1<-mean(y_obs_est*trt_est/weight-(trt_est-weight)/weight*pred.y.lasso.trt1) #trt

######for secondary set
##iptw
cvfit <-cv.glmnet(x=x_obs_est, y=trt_est,family="binomial",nfolds=5,alpha=0)
# w3 <- 1/abs(matrix(coef(cvfit, s=cvfit$lambda.min)
#                    [, 1][2:(ncol(x)+1)] ))^1
# w3[w3[,1] == Inf] <- 999999999 ## Replacing values estimated as Infinite for 999999999
# cv.lasso <- cv.glmnet(x=x_obs_est, y=trt_est, family='binomial', type.measure='auc', penalty.factor=w3)
est.parameter<-coef(cvfit, s = "lambda.min")
# fit<-grpreg(X=x_obs_est, y=trt_est, group=group, penalty="grLasso", alpha = 0.5,family="binomial")
# est.parameter<-select(fit,"AIC")$beta
weight<-(1+exp(-cbind(1,x_obs_train)%*%est.parameter))^(-1)
weight.lasso2<-weight

##regression
cvfit <-cv.glmnet(x=as.matrix(x_obs_linearreg_est), y=y_obs_est,nfolds=5,alpha=0)
est.parameter<-coef(cvfit, s = "lambda.min")
pred.y.lasso.control2<-as.matrix(cbind(1,x_obs_linearreg_control_train))%*%est.parameter
aiptw.lasso.control.2<-mean(y_obs_train*(1-trt_train)/(1-weight)-(1-trt_train-(1-weight))/(1-weight)*pred.y.lasso.control2) #control
pred.y.lasso.trt2<-as.matrix(cbind(1,x_obs_linearreg_trt_train))%*%est.parameter
aiptw.lasso.trt.2<-mean(y_obs_train*trt_train/weight-(trt_train-weight)/weight*pred.y.lasso.trt2) #trt

aiptw.lasso.control<-(aiptw.lasso.control.1+aiptw.lasso.control.2)/2
aiptw.lasso.trt<-(aiptw.lasso.trt.1+aiptw.lasso.trt.2)/2
aiptw.lasso.all<-cbind(aiptw.lasso.all,c(aiptw.lasso.control,aiptw.lasso.trt))

# ###deep learning
# trt_train_extend<-to_categorical(trt_train,num_classes = 2)
# model = keras_model_sequential() %>% 
#   layer_dense(units=6, activation="relu", input_shape=10) %>% 
#   layer_dense(units=3, activation = "relu") %>% 
#   layer_dense(units=ncol(trt_train_extend), activation="softmax")
# 
# model %>% compile(
#   loss = "categorical_crossentropy",
#   optimizer =  optimizer_rmsprop(), 
#   metrics = list("accuracy")
# )
# 
# model %>% summary()
# # TRAIN_DATA_URL<-"/Users/chixiang.chen/Library/CloudStorage/OneDrive-UniversityofMarylandSchoolofMedicine/longterm_research/causal inference machine learning/multir_ml/"
# # train_file_trt<-get_file("trt_train.csv", TRAIN_DATA_URL)
# # train_file_x<-get_file("x_obs_train.csv", TRAIN_DATA_URL)
# # train_file_trt<-"/Users/chixiang.chen/Library/CloudStorage/OneDrive-UniversityofMarylandSchoolofMedicine/longterm_research/causal inference machine learning/multir_ml/trt_train.csv"
# # train_file_x<-"/Users/chixiang.chen/Library/CloudStorage/OneDrive-UniversityofMarylandSchoolofMedicine/longterm_research/causal inference machine learning/multir_ml/x_obs_train.csv"
# # 
# # library(tfdatasets)
# # #read data
# # train_trt <- make_csv_dataset(
# #   train_file_trt, 
# #   field_delim = ",",
# #   batch_size = 5, 
# #   num_epochs = 1
# # )
# # 
# # train_trt %>% 
# #   reticulate::as_iterator() %>% 
# #   reticulate::iter_next() %>% 
# #   reticulate::py_to_r()
# # 
# # train_x <- make_csv_dataset(
# #   train_file_x, 
# #   field_delim = ",",
# #   batch_size = 5, 
# #   num_epochs = 1
# # )
# # 
# # train_x %>% 
# #   reticulate::as_iterator() %>% 
# #   reticulate::iter_next() %>% 
# #   reticulate::py_to_r()
# 
# model %>% fit(x=(x_obs_train), y=(trt_train_extend), epochs = 10,verbose = 1)
# 
# scores = model %>% evaluate(x, y, verbose = 0)
# print(scores)


###AIPTW random forest
mydata.1<-data.frame(y=as.factor(trt_train),x=x_obs_train)
mydata.2<-data.frame(y=as.factor(trt_est),x=x_obs_est)
mydata.reg.1<-data.frame(y=(y_obs_train),x=x_obs_reg_train)
mydata.reg.2<-data.frame(y=(y_obs_est),x=x_obs_reg_est)
mydata.reg.trt1<-data.frame(y=y_obs_train,x=x_obs_reg_trt_train)
mydata.reg.trt2<-data.frame(y=y_obs_est,x=x_obs_reg_trt_est)
mydata.reg.control1<-data.frame(y=y_obs_train,x=x_obs_reg_control_train)
mydata.reg.control2<-data.frame(y=y_obs_est,x=x_obs_reg_control_est)

####for first set of data
##iptw
# mtry <- tuneRF(mydata.1[,-1],mydata.1$y, ntreeTry=1000,
#                stepFactor=1.2,improve=0.01, trace=F, plot=F)
# best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
# print(mtry)
# print(best.m)
rf<-ranger(y~., num.trees=1000,data=mydata.1,replace=T,write.forest = T,probability = T)
# pred <- predict(rf, data = mydata.2)
# table(mydata.2$y, predictions(pred))

# rf <-randomForest(y~., xtest=mydata.2[,-1],ytest=mydata.2$y,
#                   ntree=1000,data=mydata.1,mtry=best.m,replace=T)
# print(rf)
# plot(rf)
# table(mydata.2$y, pred)
# importance(rf)
# varImpPlot(rf)
# plot(rf$test$votes[,2],mu_trt_assign[!(1:N %in% sample.ind)])
# weight<-rf$test$votes[,2]
weight<-predict(rf, mydata.2)$predictions[,2]
weight.rf1<-weight

##regression
rf<-ranger(y~., num.trees=1000,data=mydata.reg.1,replace=T,write.forest = T)
pred.y.rf.trt1<-predict(rf, mydata.reg.trt2)$predictions
aiptw.rf.trt.1<-mean(y_obs_est*trt_est/weight-(trt_est-weight)/weight*pred.y.rf.trt1) #trt
pred.y.rf.control1<-predict(rf, mydata.reg.control2)$predictions
aiptw.rf.control.1<-mean(y_obs_est*(1-trt_est)/(1-weight)-(1-trt_est-(1-weight))/(1-weight)*pred.y.rf.control1) #control

####for second set of data
##iptw
# mtry <- tuneRF(mydata.2[,-1],mydata.2$y, ntreeTry=1000, 
#                stepFactor=0.8,improve=0.01, trace=F, plot=F)
# best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
# print(mtry)
# print(best.m)
rf<-ranger(y~., num.trees=1000,data=mydata.2,replace=T,write.forest = T,probability = T)
# pred <- predict(rf, data = mydata.1)
# table(mydata.1$y, predictions(pred))
# rf <-randomForest(y~., xtest=mydata.1[,-1],ytest=mydata.1$y,
#                   ntree=1000,data=mydata.2,mtry=best.m,replace=T)
# print(rf)
# plot(rf)
# table(mydata.2$y, pred)
# importance(rf)
# varImpPlot(rf)
# plot(rf$test$votes[,2],mu_trt_assign[sample.ind])
weight<-predict(rf, mydata.1)$predictions[,2]
weight.rf2<-weight

##regression
rf<-ranger(y~., num.trees=1000,data=mydata.reg.2,replace=T,write.forest = T)
pred.y.rf.trt2<-predict(rf, mydata.reg.trt1)$predictions
aiptw.rf.trt.2<-mean(y_obs_train*trt_train/weight-(trt_train-weight)/weight*pred.y.rf.trt2) #trt
pred.y.rf.control2<-predict(rf, mydata.reg.control1)$predictions
aiptw.rf.control.2<-mean(y_obs_train*(1-trt_train)/(1-weight)-(1-trt_train-(1-weight))/(1-weight)*pred.y.rf.control2) #control

aiptw.rf.control<-(aiptw.rf.control.1+aiptw.rf.control.2)/2
aiptw.rf.trt<-(aiptw.rf.trt.1+aiptw.rf.trt.2)/2
aiptw.rf.all<-cbind(aiptw.rf.all,c(aiptw.rf.control,aiptw.rf.trt))

##############
#do AIPTW boosting
##############
##first data
#propensity score model
# variable names
features <- setdiff(names(mydata.1), "y")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(mydata.1, features, verbose = FALSE)
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName) 
# Prepare the training data
features_train <- vtreat::prepare(treatplan, mydata.1, varRestriction = new_vars) %>% as.matrix()
response_train <- as.numeric(mydata.1$y)-1
# Prepare the test data
features_test <- vtreat::prepare(treatplan, mydata.2, varRestriction = new_vars) %>% as.matrix()
response_test <- as.numeric(mydata.2$y)-1
dim(features_train)
dim(features_test)
#initial try
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 0 ,              # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# get number of trees that minimize error
vect=xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_logloss_mean == min(train_logloss_mean))[1],
    rmse.train   = min(train_logloss_mean),
    ntrees.test  = which(test_logloss_mean == min(test_logloss_mean))[1],
    rmse.test   = min(test_logloss_mean),
  )
#final estimate
xgb.fit.final <- xgboost(
  data = features_train,
  label = response_train,
  nrounds = as.numeric(vect[3]),
  objective = "binary:logistic",  # for regression models
  verbose = 0              # silent,
)

weight = predict(xgb.fit.final, features_test,type="response")
weight.gb1<-weight
#roc(response_test ~ weight, plot = T, print.auc = TRUE)

# mydata.1$y<-as.numeric(mydata.1$y)-1
# model_gbm = gbm(y ~.,
#                 data = mydata.1,
#                 distribution = "bernoulli",
#                 train.fraction=0.5,
#                 cv.folds = 5,
#                 shrinkage = .1,
#                 n.minobsinnode = 10,
#                 n.trees = 100,
#                 verbose=F)
# print(model_gbm)
# weight = predict.gbm(model_gbm, mydata.2[,-1],type="response")
# weight.gb1<-weight
# #boxplot(weight-mu_trt_assign[!(1:N %in% sample.ind)])

##regression
# variable names
features <- setdiff(names(mydata.reg.1), "y")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(mydata.reg.1, features, verbose = FALSE)
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName) 
# Prepare the training data
features_train <- vtreat::prepare(treatplan, mydata.reg.1, varRestriction = new_vars) %>% as.matrix()
response_train <- mydata.reg.1$y

dim(features_train)
dim(features_test)
#initial try
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:squarederror",  # for regression models
  verbose = 0 ,              # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# get number of trees that minimize error
vect=xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )
#final estimate
xgb.fit.final <- xgboost(
  data = features_train,
  label = response_train,
  nrounds = as.numeric(vect[3]),
  objective = "reg:squarederror",  # for regression models
  verbose = 0              # silent,
)

# Prepare the test data
features_test_trt2 <- vtreat::prepare(treatplan, mydata.reg.trt2, varRestriction = new_vars) %>% as.matrix()
response_test_trt2 <- mydata.reg.trt2$y
pred.y.gb.trt1 = predict(xgb.fit.final, features_test_trt2,type="response")
aiptw.gb.trt.1<-mean(y_obs_est*trt_est/weight-(trt_est-weight)/weight*pred.y.gb.trt1) #trt

features_test_control2 <- vtreat::prepare(treatplan, mydata.reg.control2, varRestriction = new_vars) %>% as.matrix()
response_test_control2 <- mydata.reg.control2$y
pred.y.gb.control1<-predict(xgb.fit.final, features_test_control2,type="response")
aiptw.gb.control.1<-mean(y_obs_est*(1-trt_est)/(1-weight)-(1-trt_est-(1-weight))/(1-weight)*pred.y.gb.control1) #control

# model_gbm = gbm(y ~.,
#                 data = mydata.reg.1,
#                 distribution = "gaussian",
#                 train.fraction=0.5,
#                 cv.folds = 5,
#                 shrinkage = .1,
#                 n.minobsinnode = 10,
#                 n.trees = 100,
#                 verbose=F)
# pred.y.gb.trt1 = predict.gbm(model_gbm, mydata.reg.trt2[,-1],type="response")
# aiptw.gb.trt.1<-mean(y_obs_est*trt_est/weight-(trt_est-weight)/weight*pred.y.gb.trt1) #trt
# pred.y.gb.control1<-predict.gbm(model_gbm, mydata.reg.control2[,-1],type="response")
# aiptw.gb.control.1<-mean(y_obs_est*(1-trt_est)/(1-weight)-(1-trt_est-(1-weight))/(1-weight)*pred.y.gb.control1) #control

###second data
#propensity score model
# variable names
features <- setdiff(names(mydata.2), "y")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(mydata.2, features, verbose = FALSE)
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName) 
# Prepare the training data
features_train <- vtreat::prepare(treatplan, mydata.2, varRestriction = new_vars) %>% as.matrix()
response_train <- as.numeric(mydata.2$y)-1
# Prepare the test data
features_test <- vtreat::prepare(treatplan, mydata.1, varRestriction = new_vars) %>% as.matrix()
response_test <- as.numeric(mydata.1$y)-1
dim(features_train)
dim(features_test)
#initial try
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 0 ,              # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# get number of trees that minimize error
vect=xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_logloss_mean == min(train_logloss_mean))[1],
    rmse.train   = min(train_logloss_mean),
    ntrees.test  = which(test_logloss_mean == min(test_logloss_mean))[1],
    rmse.test   = min(test_logloss_mean),
  )
#final estimate
xgb.fit.final <- xgboost(
  data = features_train,
  label = response_train,
  nrounds = as.numeric(vect[3]),
  objective = "binary:logistic",  # for regression models
  verbose = 0              # silent,
)

weight = predict(xgb.fit.final, features_test,type="response")
weight.gb2<-weight
#roc(response_test ~ weight, plot = T, print.auc = TRUE)

# mydata.1$y<-as.numeric(mydata.1$y)-1
# model_gbm = gbm(y ~.,
#                 data = mydata.1,
#                 distribution = "bernoulli",
#                 train.fraction=0.5,
#                 cv.folds = 5,
#                 shrinkage = .1,
#                 n.minobsinnode = 10,
#                 n.trees = 100,
#                 verbose=F)
# print(model_gbm)
# weight = predict.gbm(model_gbm, mydata.2[,-1],type="response")
# weight.gb1<-weight
# #boxplot(weight-mu_trt_assign[!(1:N %in% sample.ind)])

##regression
# variable names
features <- setdiff(names(mydata.reg.2), "y")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(mydata.reg.2, features, verbose = FALSE)
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName) 
# Prepare the training data
features_train <- vtreat::prepare(treatplan, mydata.reg.2, varRestriction = new_vars) %>% as.matrix()
response_train <- mydata.reg.2$y

dim(features_train)
dim(features_test)
#initial try
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:squarederror",  # for regression models
  verbose = 0 ,              # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# get number of trees that minimize error
vect<-xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )
#final estimate
xgb.fit.final <- xgboost(
  data = features_train,
  label = response_train,
  nrounds = as.numeric(vect[3]),
  objective = "reg:squarederror",  # for regression models
  verbose = 0              # silent,
)

# Prepare the test data
features_test_trt1 <- vtreat::prepare(treatplan, mydata.reg.trt1, varRestriction = new_vars) %>% as.matrix()
response_test_trt1 <- mydata.reg.trt1$y
pred.y.gb.trt2 = predict(xgb.fit.final, features_test_trt1,type="response")
aiptw.gb.trt.2<-mean(y_obs_train*trt_train/weight-(trt_train-weight)/weight*pred.y.gb.trt2) #trt

features_test_control1 <- vtreat::prepare(treatplan, mydata.reg.control1, varRestriction = new_vars) %>% as.matrix()
response_test_control1 <- mydata.reg.control1$y
pred.y.gb.control2<-predict(xgb.fit.final, features_test_control1,type="response")
aiptw.gb.control.2<-mean(y_obs_train*(1-trt_train)/(1-weight)-(1-trt_train-(1-weight))/(1-weight)*pred.y.gb.control2) #control

aiptw.gb.control<-(aiptw.gb.control.1+aiptw.gb.control.2)/2
aiptw.gb.trt<-(aiptw.gb.trt.1+aiptw.gb.trt.2)/2
aiptw.gb.all<-cbind(aiptw.gb.all,c(aiptw.gb.control,aiptw.gb.trt))

# 
# mydata.2$y<-as.numeric(mydata.2$y)-1
# model_gbm = gbm(y ~.,
#                 data = mydata.2,
#                 distribution = "bernoulli",
#                 train.fraction=0.5,
#                 cv.folds = 5,
#                 shrinkage = .1,
#                 n.minobsinnode = 10,
#                 n.trees = 100,
#                 verbose=F)
# print(model_gbm)
# weight = predict.gbm(model_gbm, mydata.1[,-1],type="response")
# weight.gb2<-weight
# #boxplot(weight-mu_trt_assign[!(1:N %in% sample.ind)])
# 
# ##regression
# model_gbm = gbm(y ~.,
#                 data = mydata.reg.2,
#                 distribution = "gaussian",
#                 train.fraction=0.5,
#                 cv.folds = 5,
#                 shrinkage = .1,
#                 n.minobsinnode = 10,
#                 n.trees = 100,
#                 verbose=F)
# pred.y.gb.trt2 = predict.gbm(model_gbm, mydata.reg.trt1[,-1],type="response")
# aiptw.gb.trt.2<-mean(y_obs_train*trt_train/weight-(trt_train-weight)/weight*pred.y.gb.trt2) #trt
# pred.y.gb.control2<-predict.gbm(model_gbm, mydata.reg.control1[,-1],type="response")
# aiptw.gb.control.2<-mean(y_obs_train*(1-trt_train)/(1-weight)-(1-trt_train-(1-weight))/(1-weight)*pred.y.gb.control2) #control
# 
# aiptw.gb.control<-(aiptw.gb.control.1+aiptw.gb.control.2)/2
# aiptw.gb.trt<-(aiptw.gb.trt.1+aiptw.gb.trt.2)/2
# aiptw.gb.all<-cbind(aiptw.gb.all,c(aiptw.gb.control,aiptw.gb.trt))


#######
#do multiple robust method
#first data
pstrt1<-multir.match(weight.linear=weight.linear1,weight.lasso=weight.lasso1,weight.rf=weight.rf1,weight.gb=weight.gb1,
            pred.y.linear=pred.y.linear.trt1,pred.y.lasso=pred.y.lasso.trt1,pred.y.rf=pred.y.rf.trt1,pred.y.gb=pred.y.gb.trt1,
            trt=trt_est)
mr.trt1<-sum(pstrt1*y_obs_est[trt_est==1])
pscontrol1<-multir.match(weight.linear=1-weight.linear1,weight.lasso=1-weight.lasso1,weight.rf=1-weight.rf1,weight.gb=1-weight.gb1,
                     pred.y.linear=pred.y.linear.control1,pred.y.lasso=pred.y.lasso.control1,pred.y.rf=pred.y.rf.control1,pred.y.gb=pred.y.gb.control1,
                     trt=1-trt_est)
mr.control1<-sum(pscontrol1*y_obs_est[trt_est==0])

#second data
pstrt2<-multir.match(weight.linear=weight.linear2,weight.lasso=weight.lasso2,weight.rf=weight.rf2,weight.gb=weight.gb2,
                     pred.y.linear=pred.y.linear.trt2,pred.y.lasso=pred.y.lasso.trt2,pred.y.rf=pred.y.rf.trt2,pred.y.gb=pred.y.gb.trt2,
                     trt=trt_train)
mr.trt2<-sum(pstrt2*y_obs_train[trt_train==1])
pscontrol2<-multir.match(weight.linear=1-weight.linear2,weight.lasso=1-weight.lasso2,weight.rf=1-weight.rf2,weight.gb=1-weight.gb2,
                         pred.y.linear=pred.y.linear.control2,pred.y.lasso=pred.y.lasso.control2,pred.y.rf=pred.y.rf.control2,pred.y.gb=pred.y.gb.control2,
                         trt=1-trt_train)
mr.control2<-sum(pscontrol2*y_obs_train[trt_train==0])

mr.control<-(mr.control1+mr.control2)/2
mr.trt<-(mr.trt1+mr.trt2)/2
mr.all<-cbind(mr.all,c(mr.control,mr.trt))

print(iter)
}
timeend<-Sys.time()
timeend-timestart

#output data
output<-list(raw.all=raw.all,
             aiptw.linear.all=aiptw.linear.all,
             aiptw.known.all=aiptw.known.all,
             aiptw.lasso.all=aiptw.lasso.all,
             aiptw.rf.all=aiptw.rf.all,
             aiptw.gb.all=aiptw.gb.all,
             mr.all=mr.all)


setwd("/data/mprc_data3/chixiang/mrcml/manuscript_code/psnlinear_regnlinear_n2000/")
fname<-paste0("mrcml", seed, ".rds")
saveRDS(output, file = fname)

#bias
bias=c(mean(raw.all[2,]-raw.all[1,])-as.vector(true[2]-true[1]),
mean(aiptw.linear.all[2,]-aiptw.linear.all[1,])-as.vector(true[2]-true[1]),
mean(aiptw.known.all[2,]-aiptw.known.all[1,])-as.vector(true[2]-true[1]),
mean(aiptw.lasso.all[2,]-aiptw.lasso.all[1,])-as.vector(true[2]-true[1]),
mean(aiptw.rf.all[2,]-aiptw.rf.all[1,])-as.vector(true[2]-true[1]),
mean(aiptw.gb.all[2,]-aiptw.gb.all[1,])-as.vector(true[2]-true[1]),
mean(mr.all[2,]-mr.all[1,])-as.vector(true[2]-true[1]))

#variability
mcsd=c(sd(raw.all[2,]-raw.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(aiptw.linear.all[2,]-aiptw.linear.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(aiptw.known.all[2,]-aiptw.known.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(aiptw.lasso.all[2,]-aiptw.lasso.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(aiptw.rf.all[2,]-aiptw.rf.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(aiptw.gb.all[2,]-aiptw.gb.all[1,]-as.vector(true[2]-true[1]),na.rm=T),
sd(mr.all[2,]-mr.all[1,]-as.vector(true[2]-true[1]),na.rm=T))

output=round(cbind(Bias=bias,MCSD=mcsd),3)
rownames(output)<-c("Raw","AIPW.Reg","AIPW.known","AIPW.Preg","AIPW.RF","AIPW.GB","MRCML")
output

#check normality #(betahat-beta0)/sd
mr.pool<-(mr.all[2,]-mr.all[1,]-as.vector(true[2]-true[1]))/(sd((mr.all[2,]-mr.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
aiptw.known.pool<-(aiptw.known.all[2,]-aiptw.known.all[1,]-as.vector(true[2]-true[1]))/(sd((aiptw.known.all[2,]-aiptw.known.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
aiptw.rf.pool<-(aiptw.rf.all[2,]-aiptw.rf.all[1,]-as.vector(true[2]-true[1]))/(sd((aiptw.rf.all[2,]-aiptw.rf.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
aiptw.gb.pool<-(aiptw.gb.all[2,]-aiptw.gb.all[1,]-as.vector(true[2]-true[1]))/(sd((aiptw.gb.all[2,]-aiptw.gb.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
aiptw.linear.pool<-(aiptw.linear.all[2,]-aiptw.linear.all[1,]-as.vector(true[2]-true[1]))/(sd((aiptw.linear.all[2,]-aiptw.linear.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
aiptw.lasso.pool<-(aiptw.lasso.all[2,]-aiptw.lasso.all[1,]-as.vector(true[2]-true[1]))/(sd((aiptw.lasso.all[2,]-aiptw.lasso.all[1,]-as.vector(true[2]-true[1])),na.rm=T))
raw.pool<-(raw.all[2,]-raw.all[1,]-as.vector(true[2]-true[1]))/(sd((raw.all[2,]-raw.all[1,]-as.vector(true[2]-true[1])),na.rm=T))

# Normal curve
# X-axis grid
x2 <- seq(-5, 5, length = 100)
fun <- dnorm(x2, mean = mean(x), sd = sd(x))
hist(mr.pool,prob = TRUE,xlim=c(-5,5),ylim=c(0,0.6),main="MRCML",xlab="",nclass=30)
lines(x2, fun, col = 1, lwd = 2)
hist(aiptw.rf.pool,prob = TRUE,xlim=c(-5,5),ylim=c(0,0.6),main="AIPTW.RF",xlab="",nclass=30)
lines(x2, fun, col = 1, lwd = 2)
hist(aiptw.linear.pool,prob = TRUE,xlim=c(-5,5),ylim=c(0,0.6),main="AIPTW.Reg",xlab="",nclass=40)
lines(x2, fun, col = 1, lwd = 2)
hist(aiptw.lasso.pool,prob = TRUE,xlim=c(-5,5),ylim=c(0,0.6),main="AIPTW.Preg",xlab="",nclass=20)
lines(x2, fun, col = 1, lwd = 2)
hist(aiptw.gb.pool,prob = TRUE,xlim=c(-5,5),ylim=c(0,0.6),main="AIPTW.GB",xlab="",nclass=30)
lines(x2, fun, col = 1, lwd = 2)
# 
# 
# hist(aiptw.rf.all[2,]-aiptw.rf.all[1,])
# qqnorm((aiptw.rf.all[2,]-aiptw.rf.all[1,]))
# qqline((aiptw.rf.all[2,]-aiptw.rf.all[1,]))
# 
# index.keep<-which(apply(iptw.lasso.all,2,function(x)
# {ifelse(mean(abs(x))>5,1,0)}
# )==0)
# apply(iptw.lasso.all[,index.keep],1,mean)-true
# 
# 
#       